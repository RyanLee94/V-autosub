#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ›´æ–°/åˆå¹¶è®¢é˜…è„šæœ¬ï¼ˆé€‚åˆåœ¨ GitHub Actions ä¸­è¿è¡Œï¼‰
åŠŸèƒ½ï¼š
- ä» SOURCE_LISTING_URL æŠ“å–è®¢é˜…é¡µé¢ä¸­çš„è®¢é˜…é“¾æ¥ï¼ˆæˆ–ä½ ä¹Ÿå¯ä»¥ç›´æ¥æŠŠè®¢é˜…åˆ—è¡¨å†™æ­»åˆ° SUB_LIST ä¸­ï¼‰
- é€ä¸ªä¸‹è½½è®¢é˜…å†…å®¹ï¼Œè§£æå‡º vmess/trojan/hysteria2 èŠ‚ç‚¹
- è§£æèŠ‚ç‚¹å¤‡æ³¨ä¸­çš„â€œå‰©ä½™æµé‡â€ï¼Œä»…ä¿ç•™å‰©ä½™æµé‡ > MIN_REMAIN_GB çš„èŠ‚ç‚¹
- å¯¹é€šè¿‡æµé‡ç­›é€‰çš„èŠ‚ç‚¹åš TCP è¿æ¥æµ‹é€Ÿï¼ˆå¹¶å‘ï¼‰ï¼Œä»…ä¿ç•™å»¶è¿Ÿ <= MAX_LATENCY çš„èŠ‚ç‚¹
- è¾“å‡ºåˆå¹¶è®¢é˜…æ–‡ä»¶ OUTPUT_FILEï¼ˆæ¯è¡Œä¸€ä¸ªèŠ‚ç‚¹ï¼‰
"""
import requests
import re
import time
import base64
import socket
import json
import math
import sys
from urllib.parse import urlparse, unquote
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Tuple, Optional, Set

# ================ é…ç½® ================
SOURCE_LISTING_URL = "https://v2raya.net/free-nodes/2025-02-02-free-v2ray-node-subscriptions.html"
FETCH_RETRIES = 6
FETCH_WAIT_SECONDS = 6   # åŸºæœ¬é‡è¯•ç­‰å¾…ï¼ˆä¼šåšæŒ‡æ•°é€€é¿ï¼‰
FETCH_TIMEOUT = 20
SUB_TIMEOUT = 15

TCP_TIMEOUT = 8          # å•èŠ‚ç‚¹ TCP è¿æ¥è¶…æ—¶ï¼ˆç§’ï¼‰
MAX_LATENCY = float(int(10))  # ç§’ï¼Œè¶…è¿‡åˆ™ä¸¢å¼ƒ
MIN_REMAIN_GB = float(5.0)    # ä¿ç•™æµé‡é˜ˆå€¼ï¼ˆGBï¼‰ï¼Œå¯ä¿®æ”¹æˆ–é€šè¿‡ç¯å¢ƒå˜é‡ä¼ å…¥
OUTPUT_FILE = "sub.txt"
HEADERS = {"User-Agent": "Mozilla/5.0 (GitHub Actions)"}
MAX_WORKERS = 30        # å¹¶å‘çº¿ç¨‹æ•°ï¼ˆæµ‹é€Ÿç”¨ï¼‰

# æ­£åˆ™ï¼šåŒ¹é…å¯èƒ½çš„èŠ‚ç‚¹è¡Œï¼ˆç®€å•ç­›é€‰ï¼‰
NODE_PATTERN = re.compile(r'(vmess://[A-Za-z0-9+/=._-]+|trojan://[^\s\'"<>]+|hysteria2://[^\s\'"<>]+)', re.IGNORECASE)

# ================ å·¥å…·å‡½æ•° ================

def _b64_fix_padding(s: str) -> bytes:
    """ä¿®æ­£ base64 padding å¹¶ decodeï¼Œè¿”å› bytesï¼›è‹¥å¤±è´¥æŠ›å‡ºå¼‚å¸¸"""
    s = s.strip()
    # remove possible URI-safe characters
    # try standard base64
    padding = len(s) % 4
    if padding:
        s += '=' * (4 - padding)
    return base64.b64decode(s)

def fetch_with_retry(url: str, retries=FETCH_RETRIES, timeout=FETCH_TIMEOUT, headers=HEADERS) -> str:
    last_exc = None
    for i in range(1, retries+1):
        try:
            print(f"[fetch] ({i}/{retries}) GET {url}")
            r = requests.get(url, headers=headers, timeout=timeout)
            r.raise_for_status()
            return r.text
        except Exception as e:
            last_exc = e
            wait = FETCH_WAIT_SECONDS * (2 ** (i-1))  # æŒ‡æ•°é€€é¿
            print(f"  -> fetch failed: {e!r}. wait {wait}s then retry.")
            time.sleep(wait)
    raise RuntimeError(f"Failed to fetch {url} after {retries} attempts") from last_exc

def extract_links_from_listing(html: str) -> List[str]:
    """ä»æºé¡µä¸­æå–æ‰€æœ‰ http(s) é“¾æ¥ï¼ˆå»é‡ï¼‰ï¼Œåç»­æ ¹æ®å†…å®¹ç­›é€‰ä¸ºè®¢é˜…é“¾æ¥"""
    raw_links = re.findall(r'https?://[^\s"\'<>]+', html)
    # ä¸€èˆ¬é¡µé¢å†…å¯èƒ½å¸¦æœ‰å¾ˆå¤šé™æ€èµ„æºé“¾æ¥ï¼Œå…ˆå»é‡å¹¶è¿”å›
    uniq = list(dict.fromkeys(raw_links))
    print(f"[extract_links] total links found: {len(uniq)}")
    return uniq

def is_likely_subscription_content(text: str) -> bool:
    """åˆ¤æ–­æŸä¸ªä¸‹è½½åˆ°çš„æ–‡æœ¬æ˜¯å¦å¯èƒ½æ˜¯è®¢é˜…å†…å®¹ï¼ˆå« vmess/trojan/hysteria æˆ– base64 ç¼–ç åçš„è¿™äº›ï¼‰"""
    if "vmess://" in text or "trojan://" in text or "hysteria2://" in text:
        return True
    # æœ‰äº›è®¢é˜…æ˜¯ base64 ç¼–ç çš„ï¼Œè§£ç åå¯èƒ½åŒ…å« vmess:// ç­‰
    try:
        dec = _b64_fix_padding(text)
        txt = dec.decode('utf-8', errors='ignore')
        if "vmess://" in txt or "trojan://" in txt or "hysteria2://" in txt:
            return True
    except Exception:
        pass
    # ä¹Ÿå¯èƒ½æ˜¯ json base64ï¼ˆvmess å•ç‹¬çš„ base64 jsonï¼‰ï¼Œä½†è¿™é‡Œåšé€šç”¨åˆ¤æ–­å³å¯
    return False

def extract_nodes_from_text(text: str) -> List[str]:
    """ä»ä¸€ä¸ªæ–‡æœ¬ä¸­æŠ½å– vmess/trojan/hysteria2 èŠ‚ç‚¹ï¼ˆä¸è§£ç  vmess base64 JSON çš„å†…å®¹ï¼‰
       text å¯èƒ½å·²ç»æ˜¯è§£ç åçš„æ–‡æœ¬ï¼Œä¹Ÿå¯èƒ½æ˜¯ç›´æ¥çš„è®¢é˜…æ–‡æœ¬"""
    nodes = set()
    # ç›´æ¥å¯»æ‰¾è¡Œå†…çš„ vmess:// trojan:// hysteria2://
    for m in NODE_PATTERN.finditer(text):
        nodes.add(m.group(0).strip())

    # æœ‰äº›è®¢é˜…æ˜¯æŠŠæ•´æ®µèŠ‚ç‚¹åˆ—è¡¨ç”¨ base64 ç¼–ç åœ¨ body é‡Œï¼Œæ­¤æ—¶å°è¯• base64 è§£ç å¹¶å†æå–
    try:
        dec = _b64_fix_padding(text)
        s = dec.decode('utf-8', errors='ignore')
        for m in NODE_PATTERN.finditer(s):
            nodes.add(m.group(0).strip())
    except Exception:
        pass

    # return list
    return list(nodes)

def decode_vmess_json_from_node(vmess_node: str) -> Optional[dict]:
    """ç»™å®š vmess://base64json è¿”å› dictï¼ˆæˆ– Noneï¼‰"""
    try:
        payload = vmess_node[8:].strip()
        raw = _b64_fix_padding(payload)
        s = raw.decode('utf-8', errors='ignore')
        # some vmess servers encode as JSON or as base64(json)
        # parse json safely
        data = json.loads(s)
        return data
    except Exception:
        # æœ‰äº› vmess èŠ‚ç‚¹æœ¬èº«æ˜¯ vmess://{...}ï¼ˆrareï¼‰ï¼Œè¯•ä¸€ä¸‹ç›´æ¥å»æ‰ vmess:// å¹¶ json.loads
        try:
            if vmess_node.startswith("vmess://"):
                alt = vmess_node[8:].strip()
                data = json.loads(alt)
                return data
        except Exception:
            return None
    return None

def parse_node_remark_and_remaining(node: str) -> Tuple[Optional[str], Optional[int]]:
    """
    è§£æèŠ‚ç‚¹çš„å¤‡æ³¨ï¼ˆremark/psï¼‰å’Œå¤‡æ³¨ä¸­å¯èƒ½åŒ…å«çš„â€œå‰©ä½™æµé‡â€
    è¿”å› (remark_text, remaining_bytes_or_None)
    """
    remark = None
    remain_bytes = None

    try:
        if node.startswith("vmess://"):
            info = decode_vmess_json_from_node(node)
            if info:
                remark = info.get("ps") or info.get("remark") or info.get("ps") or info.get("remarks")
        else:
            # trojan, hysteria2 ç­‰ URI å½¢å¼ï¼šfragment æˆ– userinfo ä¸­å¯èƒ½æœ‰å¤‡æ³¨ï¼ˆåç¼€ #remarkï¼‰
            u = urlparse(node)
            # fragment is the part after '#'
            if u.fragment:
                remark = unquote(u.fragment)
            # sometimes remark is appended as query param or at end of path
            if not remark:
                # trojan://password@host:port#remark -> fragment handled above
                # try URL-decoded whole node after '#'
                if "#" in node:
                    remark = unquote(node.split("#", 1)[1])
    except Exception:
        pass

    if remark:
        # å¯»æ‰¾ä¸­æ–‡â€œå‰©ä½™æµé‡ï¼š20.55 GBâ€ æˆ–è‹±æ–‡ "Remaining: 20.55 GB" ç­‰
        # æ”¯æŒå•ä½ï¼šB, KB, MB, GB, TBï¼ˆå«å¤§å°å†™ï¼ŒåŒ…å«ä¸­æ–‡ç©ºæ ¼ï¼‰
        m = re.search(r'å‰©ä½™æµé‡[:ï¼š]\s*([0-9,.]+)\s*([KMGT]?B|[KMGT]?b|[KMGT]|GB|MB|KB|TB)', remark, re.IGNORECASE)
        if not m:
            # å…¼å®¹è‹±æ–‡æˆ–ç®€å†™ï¼Œä¾‹å¦‚ "å‰©ä½™: 20.55G", "Remaining:20.5 GB"
            m = re.search(r'(å‰©ä½™|remaining|remain)[:ï¼š]?\s*([0-9,.]+)\s*([KMGT]?B|[KMGT]?b|[KMGT]|GB|MB|KB|TB)', remark, re.IGNORECASE)
            if m:
                num = m.group(2)
                unit = m.group(3)
                # unify
                try:
                    remain_bytes = convert_size_to_bytes(num, unit)
                except Exception:
                    remain_bytes = None
        else:
            num = m.group(1)
            unit = m.group(2)
            try:
                remain_bytes = convert_size_to_bytes(num, unit)
            except Exception:
                remain_bytes = None

    return remark, remain_bytes

def convert_size_to_bytes(num_str: str, unit_str: str) -> int:
    """æŠŠè¯¸å¦‚ '20.55' + 'GB' è½¬æˆå­—èŠ‚æ•´æ•°"""
    # æ¸…ç†æ•°å­—ï¼ˆå«é€—å·ï¼‰
    num = float(num_str.replace(',', ''))
    unit = unit_str.upper().replace('.', '')
    # å¸¸è§å•ä½æ˜ å°„
    if unit in ('B', ''):
        mul = 1
    elif unit in ('K', 'KB'):
        mul = 1024
    elif unit in ('M', 'MB'):
        mul = 1024 ** 2
    elif unit in ('G', 'GB'):
        mul = 1024 ** 3
    elif unit in ('T', 'TB'):
        mul = 1024 ** 4
    else:
        mul = 1
    return int(num * mul)

def parse_host_port(node: str) -> Tuple[Optional[str], Optional[int]]:
    """å°½é‡è§£æå‡ºä¸»æœºå’Œç«¯å£ï¼Œä¾› TCP æµ‹é€Ÿä½¿ç”¨"""
    try:
        if node.startswith("vmess://"):
            info = decode_vmess_json_from_node(node)
            if info:
                host = info.get("add") or info.get("address") or info.get("host")
                port = info.get("port")
                try:
                    port = int(port)
                except Exception:
                    port = None
                return host, port
        else:
            u = urlparse(node)
            # urlparse for trojan will parse hostname and port
            return u.hostname, (u.port if u.port else None)
    except Exception:
        pass
    return None, None

def tcp_test(host: str, port: int, timeout=TCP_TIMEOUT) -> Optional[float]:
    """å¯¹æŒ‡å®š host:port åš TCP å»ºè¿æµ‹è¯•ï¼Œè¿”å›è€—æ—¶ï¼ˆç§’ï¼‰ï¼Œå¤±è´¥è¿”å› None"""
    if not host or not port:
        return None
    try:
        start = time.time()
        sock = socket.create_connection((host, int(port)), timeout=timeout)
        sock.close()
        return time.time() - start
    except Exception:
        return None

# ================ ä¸»æµç¨‹ ================

def main():
    print("ğŸš€ å¼€å§‹åˆå¹¶ä¸ç­›é€‰è®¢é˜…èŠ‚ç‚¹")
    print(f"æºé¡µé¢: {SOURCE_LISTING_URL}")
    try:
        listing_html = fetch_with_retry(SOURCE_LISTING_URL)
    except Exception as e:
        print(f"âŒ æ— æ³•è·å–æºé¡µé¢ï¼š{e}")
        sys.exit(1)

    candidate_links = extract_links_from_listing(listing_html)

    # ç¬¬äºŒæ­¥ï¼šé€ä¸ªå°è¯•è¿™äº›é“¾æ¥ï¼Œä¿ç•™é‚£äº›è¿”å›çš„å†…å®¹çœ‹èµ·æ¥åƒè®¢é˜…çš„é“¾æ¥
    subscription_urls = []
    for link in candidate_links:
        # è¿‡æ»¤ä¸€äº›é™æ€èµ„æºï¼ˆç®€å•è§„åˆ™ï¼‰
        if any(link.lower().endswith(ext) for ext in ('.css', '.js', '.png', '.jpg', '.jpeg', '.svg', '.ico', '.woff', '.ttf')):
            continue
        # åªä¿ç•™ http/https
        if not link.lower().startswith(('http://', 'https://')):
            continue
        try:
            txt = requests.get(link, headers=HEADERS, timeout=SUB_TIMEOUT).text
            if is_likely_subscription_content(txt):
                subscription_urls.append(link)
                print(f"  [OK] subscription candidate: {link}")
        except Exception as e:
            # å¿½ç•¥è¯·æ±‚å¤±è´¥çš„é“¾æ¥
            # print(f"  [skip] {link} -> {e}")
            continue

    # å¦‚æœæ²¡æœ‰ä»é¡µé¢è‡ªåŠ¨è¯†åˆ«åˆ°è®¢é˜…é“¾æ¥ï¼Œä¹Ÿå¯ä»¥æ‰‹åŠ¨åœ¨è¿™é‡Œè¡¥å……ï¼ˆSUB_LISTï¼‰
    if not subscription_urls:
        print("âš ï¸ æœªåœ¨é¡µé¢ä¸­è‡ªåŠ¨è¯†åˆ«åˆ°è®¢é˜…é“¾æ¥ã€‚è¯·æ£€æŸ¥é¡µé¢æ˜¯å¦é€šè¿‡ JS åŠ¨æ€ç”Ÿæˆæˆ–æ‰‹åŠ¨è®¾ç½®è®¢é˜…åˆ—è¡¨ã€‚")
        # é€€å‡ºæˆ–ç»§ç»­ï¼Ÿè¿™é‡Œé€€å‡º
        sys.exit(1)

    print(f"ğŸ”— è¯†åˆ«åˆ°è®¢é˜…é“¾æ¥æ•°é‡: {len(subscription_urls)}")

    # ä»æ¯ä¸ªè®¢é˜…é“¾æ¥ä¸­æŠ½å–èŠ‚ç‚¹
    all_nodes: Set[str] = set()
    # åŒæ—¶è®°å½•æ¯ä¸ªè®¢é˜…é“¾æ¥ä¸­çš„èŠ‚ç‚¹ä¸å…¶è§£æåˆ°çš„å‰©ä½™æµé‡ï¼ˆç”¨äºæŒ‰è®¢é˜…ä¿ç•™æˆ–è°ƒè¯•ï¼‰
    subs_info = {}

    for su in subscription_urls:
        try:
            txt = requests.get(su, headers=HEADERS, timeout=SUB_TIMEOUT).text.strip()
        except Exception as e:
            print(f"  [warn] æ— æ³•ä¸‹è½½è®¢é˜… {su}: {e}")
            continue
        nodes = extract_nodes_from_text(txt)
        print(f"  [sub] {su} -> nodes found: {len(nodes)}")
        subs_info[su] = nodes
        for n in nodes:
            all_nodes.add(n)

    print(f"ğŸ“¦ å»é‡åæ€»èŠ‚ç‚¹æ•°: {len(all_nodes)}")

    # å¯¹æ¯ä¸ªèŠ‚ç‚¹è§£æå¤‡æ³¨å’Œå‰©ä½™æµé‡ï¼Œå…ˆæŒ‰â€œå‰©ä½™æµé‡ > MIN_REMAIN_GBâ€ ç­›é€‰
    min_remain_bytes = int(MIN_REMAIN_GB * 1024 ** 3)
    candidate_nodes = []  # å­˜ (node, remain_bytes, remark)
    for node in all_nodes:
        remark, remain_bytes = parse_node_remark_and_remaining(node)
        # å¦‚æœæ²¡æœ‰è§£æåˆ°æµé‡ä¿¡æ¯ï¼Œæœ‰ä¸¤ç§ç­–ç•¥ï¼šä¸¢å¼ƒæˆ–ä¿ç•™ç­‰å¾…æµ‹é€Ÿ
        # è¿™é‡Œæˆ‘ä»¬åªä¿ç•™æ˜ç¡®è§£æåˆ°æµé‡ä¸” >= MIN çš„èŠ‚ç‚¹
        if remain_bytes is not None and remain_bytes >= min_remain_bytes:
            candidate_nodes.append((node, remain_bytes, remark))
        # è‹¥ä½ æƒ³ä¿ç•™é‚£äº›æ²¡æœ‰æµé‡ä¿¡æ¯çš„èŠ‚ç‚¹ï¼Œè¯·æŠŠä¸‹é¢æ³¨é‡Šå–æ¶ˆï¼š
        # else:
        #     candidate_nodes.append((node, remain_bytes, remark))

    print(f"ğŸ” ç¬¦åˆæµé‡é˜ˆå€¼ï¼ˆ>={MIN_REMAIN_GB} GBï¼‰çš„èŠ‚ç‚¹æ•°: {len(candidate_nodes)}")

    if not candidate_nodes:
        print("âŒ æœªæ‰¾åˆ°æ»¡è¶³æµé‡æ¡ä»¶çš„èŠ‚ç‚¹ã€‚é€€å‡ºã€‚")
        sys.exit(0)

    # å¹¶å‘ TCP æµ‹é€Ÿï¼ˆæŒ‰ host:portï¼‰
    def _test_item(item):
        node, remain_bytes, remark = item
        host, port = parse_host_port(node)
        if not host or not port:
            return None
        latency = tcp_test(host, port, timeout=TCP_TIMEOUT)
        return (latency, node, remain_bytes, remark)

    usable = []
    with ThreadPoolExecutor(max_workers=min(MAX_WORKERS, len(candidate_nodes) or 1)) as ex:
        futures = {ex.submit(_test_item, it): it for it in candidate_nodes}
        for fut in as_completed(futures):
            try:
                result = fut.result()
            except Exception:
                continue
            if not result:
                continue
            latency, node, remain_bytes, remark = result
            if latency is not None and latency <= MAX_LATENCY:
                usable.append((latency, node, remain_bytes, remark))
            else:
                # å¯é€‰ï¼šè®°å½•ä¸å¯ç”¨æˆ–é«˜å»¶è¿Ÿçš„èŠ‚ç‚¹
                pass

    usable.sort(key=lambda x: x[0])  # æŒ‰å»¶è¿Ÿæ’åº
    print(f"âœ… æœ€ç»ˆé€šè¿‡æµé‡ä¸å»¶è¿Ÿç­›é€‰çš„èŠ‚ç‚¹æ•°: {len(usable)}")

    if not usable:
        print("âŒ æ²¡æœ‰å¯ç”¨èŠ‚ç‚¹é€šè¿‡ç­›é€‰ï¼Œé€€å‡ºã€‚")
        sys.exit(0)

    # è¾“å‡ºåˆå¹¶è®¢é˜…ï¼ˆæ¯è¡Œä¸€ä¸ªèŠ‚ç‚¹ï¼‰
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        for latency, node, remain, remark in usable:
            f.write(node + "\n")

    print(f"ğŸ‰ è¾“å‡ºå†™å…¥: {OUTPUT_FILE}")
    print("å®Œæˆã€‚")

if __name__ == "__main__":
    main()
